"""
Machine Learning for Smarter Crop Planning
Proof of Concept with GaussianNB

This file implements Section 4.4: Proof of Concept using scikit-learn's GaussianNB
on the small 15-sample dataset. This validates our manual calculations with a
production ML library.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB


# ============================================================================
# DATA PREPARATION
# ============================================================================


def load_small_dataset():
    """
    Loads the 15-sample rice/maize dataset.

    Returns:
        DataFrame with features and crop labels
    """
    data = pd.DataFrame(
        {
            "N": [90, 85, 60, 74, 78, 69, 69, 94, 89, 71, 61, 80, 73, 61, 68],
            "P": [42, 58, 55, 35, 42, 37, 55, 53, 54, 54, 44, 43, 58, 38, 41],
            "K": [43, 41, 44, 40, 42, 42, 38, 40, 38, 16, 17, 16, 21, 20, 16],
            "temperature": [
                20.87974371,
                21.77046169,
                23.00445915,
                26.49109635,
                20.13017482,
                23.05804872,
                22.70883798,
                20.27774362,
                24.51588066,
                22.61359953,
                26.10018422,
                23.55882094,
                19.97215954,
                18.47891261,
                21.77689322,
            ],
            "humidity": [
                82.00274423,
                80.31964408,
                82.3207629,
                80.15836264,
                81.60487287,
                83.37011772,
                82.63941394,
                82.89408619,
                83.5352163,
                63.69070564,
                71.57476937,
                71.59351368,
                57.68272924,
                62.69503871,
                57.80840636,
            ],
            "ph": [
                6.502985292,
                7.038096361,
                7.840207144,
                6.980400905,
                7.628472891,
                7.073453503,
                5.70080568,
                5.718627178,
                6.685346424,
                5.749914421,
                6.931756558,
                6.657964753,
                6.596060648,
                5.970458434,
                6.158830619,
            ],
            "rainfall": [
                202.9355362,
                226.6555374,
                263.9642476,
                242.8640342,
                262.7173405,
                251.0549998,
                271.3248604,
                241.9741949,
                230.4462359,
                87.75953857,
                102.2662445,
                66.71995467,
                60.65171481,
                65.43835393,
                102.0861694,
            ],
            "crop": [
                "rice",
                "rice",
                "rice",
                "rice",
                "rice",
                "rice",
                "rice",
                "rice",
                "rice",
                "maize",
                "maize",
                "maize",
                "maize",
                "maize",
                "maize",
            ],
        }
    )
    return data


def discretize_features(data):
    """
    Discretizes continuous features into categorical bins to match manual approach.

    Args:
        data: DataFrame with continuous features

    Returns:
        DataFrame with discretized features
    """
    bin_edges = {
        "N": [0, 69.0, 78.48, float("inf")],
        "P": [0, 42.0, 54.0, float("inf")],
        "K": [0, 20.62, 40.24, float("inf")],
        "temperature": [0, 21.4319888576, 23.0173206468, float("inf")],
        "humidity": [0, 71.5863908422, 82.0790687108, float("inf")],
        "ph": [0, 6.37220651626, 6.94343120128, float("inf")],
        "rainfall": [0, 102.197815962, 242.187756332, float("inf")],
    }

    bin_labels = ["low", "med", "high"]

    data_binned = data.copy()
    for col, edges in bin_edges.items():
        data_binned[col] = pd.cut(
            data[col], bins=edges, labels=bin_labels, include_lowest=True
        )

    return data_binned


# ============================================================================
# GAUSSIAN NAIVE BAYES IMPLEMENTATION
# ============================================================================


def train_gaussian_nb(data):
    """
    Trains a Gaussian Naive Bayes classifier on the discretized dataset.

    Args:
        data: DataFrame with discretized features

    Returns:
        Tuple of (trained_model, label_encoders)
    """
    # Encode categorical features as integers
    label_encoders = {}
    data_encoded = data.copy()

    for column in data.columns:
        le = LabelEncoder()
        data_encoded[column] = le.fit_transform(data[column])
        label_encoders[column] = le

    # Separate features and labels
    X = data_encoded.drop(columns=["crop"])
    y = data_encoded["crop"]

    # Train model
    model = GaussianNB()
    model.fit(X, y)

    return model, label_encoders


def predict_crop(model, label_encoders, test_conditions):
    """
    Predicts crop for given test conditions.

    Args:
        model: Trained GaussianNB model
        label_encoders: Dictionary of LabelEncoders
        test_conditions: Dictionary of feature values

    Returns:
        Tuple of (predicted_crop, posterior_probabilities)
    """
    # Create test DataFrame
    test_data = pd.DataFrame([test_conditions])

    # Encode test data
    for column in test_data.columns:
        test_data[column] = label_encoders[column].transform(test_data[column])

    # Predict
    predicted_crop_encoded = model.predict(test_data)[0]
    posterior_probs = model.predict_proba(test_data)[0]

    # Decode prediction
    predicted_crop = label_encoders["crop"].inverse_transform([predicted_crop_encoded])[
        0
    ]

    return predicted_crop, posterior_probs


# ============================================================================
# VISUALIZATION
# ============================================================================


def plot_feature_distributions(data):
    """
    Plots feature distributions for rice vs maize.

    Args:
        data: Original DataFrame with continuous features
    """
    features = ["N", "P", "K", "temperature", "humidity", "ph", "rainfall"]
    colors = {"rice": "green", "maize": "orange"}

    plt.figure(figsize=(10, 10))
    for i, feature in enumerate(features, 1):
        plt.subplot(3, 3, i)
        for crop in data["crop"].unique():
            sns.kdeplot(
                data=data[data["crop"] == crop],
                x=feature,
                label=crop,
                fill=True,
                alpha=0.3,
                color=colors[crop],
                linewidth=2,
            )
        plt.title(f"{feature.capitalize()} Distribution")
        plt.xlabel(f"{feature.capitalize()} Value")
        plt.ylabel("Density")
        plt.legend()

    plt.suptitle("Feature Value Distributions â€“ Rice vs Maize", fontsize=16, y=1.02)
    plt.tight_layout()
    plt.show()


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("GAUSSIAN NAIVE BAYES - PROOF OF CONCEPT")
    print("Section 4.4: Small Dataset Validation")
    print("=" * 70)
    print()

    # Load data
    print("Loading 15-sample dataset...")
    data = load_small_dataset()
    print(f"Loaded {len(data)} samples")
    print(f"Crops: {', '.join(data['crop'].unique())}")
    print()

    # Display sample data
    print("Sample data (first 5 rows):")
    print(data.head())
    print()

    # Discretize features
    print("Discretizing features to match manual approach...")
    data_binned = discretize_features(data)
    print("Discretization complete!")
    print()

    # Train model
    print("Training Gaussian Naive Bayes classifier...")
    model, label_encoders = train_gaussian_nb(data_binned)
    print("Training complete!")
    print()

    # Define test case (matching manual example)
    print("-" * 70)
    print("TEST CASE PREDICTION")
    print("-" * 70)

    test_conditions = {
        "N": "high",
        "P": "low",
        "K": "med",
        "temperature": "med",  # "cool" mapped to "med" due to discretization
        "humidity": "med",
        "ph": "low",  # "acidic" mapped to "low"
        "rainfall": "med",
    }

    print("Test conditions:")
    for feature, value in test_conditions.items():
        print(f"  {feature}: {value}")
    print()

    # Make prediction
    predicted_crop, posteriors = predict_crop(model, label_encoders, test_conditions)

    print("RESULTS:")
    print("-" * 70)
    print(f"Predicted Crop: {predicted_crop.upper()}")
    print()
    print("Posterior Probabilities:")
    for crop_name, prob in zip(label_encoders["crop"].classes_, posteriors):
        print(f"  {crop_name}: {prob:.3f} ({prob * 100:.1f}%)")
    print()

    # Comparison with manual calculation
    print("=" * 70)
    print("COMPARISON WITH MANUAL CALCULATION")
    print("=" * 70)
    print("Manual calculation results (from Section 3):")
    print("  rice:  0.978 (97.8%)")
    print("  maize: 0.022 (2.2%)")
    print()
    print("GaussianNB results:")
    for crop_name, prob in zip(label_encoders["crop"].classes_, posteriors):
        print(f"  {crop_name}: {prob:.3f} ({prob * 100:.1f}%)")
    print()
    print("Note: Minor differences are expected due to discretization and")
    print("Gaussian assumption in GaussianNB vs. manual categorical approach.")
    print()

    # Visualization
    print("-" * 70)
    print("Generating feature distribution plots...")
    print("-" * 70)
    plot_feature_distributions(data)

    print()
    print("=" * 70)
    print("Analysis complete!")
    print("=" * 70)
    print()
    print("KEY FINDINGS:")
    print("- Clear feature separation between rice and maize")
    print("- High humidity and rainfall strongly indicate rice")
    print("- Low potassium (K) strongly indicates maize")
    print("- GaussianNB successfully validates manual approach")
